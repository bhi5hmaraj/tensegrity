# Experiment 01: Baseline Validation

## Status: ⊙ Design Complete, Implementation Pending

**Date designed**: 2025-11-22
**Implemented**: No
**Data collected**: No
**Analysis complete**: No

---

## 1. Hypothesis

**H0 (Null)**: The simulation exhibits unstable dynamics (H diverges or oscillates chaotically).

**H1 (Alternative)**: The simulation reaches statistical equilibrium with H oscillating around a stable mean.

**Testable prediction**: After initial transient (~20 steps), H(t) is a stationary time series with constant mean and finite variance.

---

## 2. Purpose

**Primary**: Sanity check that the model exhibits reasonable behavior before testing more complex hypotheses.

**Secondary**:
- Establish baseline values for H, T, V, field ranges
- Verify energy calculations don't produce NaN, inf, or negative values
- Calibrate governance thresholds (e.g., H_emergency = 2× mean(H))
- Identify confusions in implementation

**Success = green light** to proceed to Exp02-Exp05.
**Failure = red light** to debug model before claiming anything.

---

## 3. Starting State

### 3.1 Graph Topology

**Nodes**: 6
```python
nodes = ['A_core', 'B_api', 'C_db', 'D_featureX', 'E_featureY', 'F_util']
```

**Edges**: Hub-and-spoke (A_core is hub)
```python
edges = [
    ('A_core', 'B_api', 0.9),      # Tight coupling
    ('A_core', 'C_db', 0.7),
    ('A_core', 'F_util', 0.4),
    ('B_api', 'D_featureX', 0.6),
    ('B_api', 'E_featureY', 0.5),
    ('C_db', 'D_featureX', 0.3),
]
```

**Justification**: Simple topology, identifiable hub, manageable size.

### 3.2 Initial Field Values

```python
health = {
    'A_core': 0.8,
    'B_api': 0.8,
    'C_db': 0.7,
    'D_featureX': 0.6,
    'E_featureY': 0.6,
    'F_util': 0.7,
}

complexity = {
    'A_core': 0.7,
    'B_api': 0.6,
    'C_db': 0.5,
    'D_featureX': 0.4,
    'E_featureY': 0.4,
    'F_util': 0.3,
}

risk = {
    'A_core': 0.3,
    'B_api': 0.3,
    'C_db': 0.3,
    'D_featureX': 0.3,
    'E_featureY': 0.3,
    'F_util': 0.3,
}

demand = {
    'A_core': 0.4,
    'B_api': 0.5,
    'C_db': 0.3,
    'D_featureX': 0.7,  # Popular feature
    'E_featureY': 0.5,
    'F_util': 0.2,
}
```

**Justification**: Moderate health (0.6-0.8), varied demand (0.2-0.7), uniform risk for simplicity.

### 3.3 Actors

```python
actors = [
    FeatureEngineer(name='Alice', business_weight=0.8, stability_weight=0.2),
    RefactorEngineer(name='Bob', business_weight=0.2, stability_weight=0.8),
    AIAgent(name='Agent-1', feature_bias=0.6),
]
```

**Activity**: Each actor chooses one action per time step.

### 3.4 Scheduled Events

**None**. Baseline has no external shocks.

### 3.5 Run Parameters

- **n_steps**: 100
- **random_seed**: 42 (for reproducibility)

---

## 4. Measurement Protocol

### 4.1 What to Log (Every Step)

**Global metrics**:
```python
log_entry = {
    'step': int,
    'H': float,        # Hamiltonian
    'T': float,        # Kinetic energy
    'V': float,        # Total potential
    'V_struct': float, # Structural potential
    'V_bus': float,    # Business potential
    'L': float,        # Lagrangian (T - V)
}
```

**Per-node metrics**:
```python
node_entry = {
    'node': str,
    'health': float,
    'complexity': float,
    'risk': float,
    'demand': float,
    'bad': float,      # Derived badness
    'E_local': float,  # Local Dirichlet energy
    'grad_V': float,   # Gradient magnitude
}
```

**Events**:
```python
event_log = {
    'step': int,
    'actor': str,
    'event_type': str,  # 'FeatureChange', 'Refactor', etc.
    'target_node': str,
}
```

**Incidents** (if any):
```python
incident_log = {
    'step': int,
    'node': str,
    'type': str,  # E.g., 'health_critical', 'complexity_overflow'
}
```

### 4.2 Output Files

- `baseline_timeseries.csv`: Global metrics (H, T, V, ...)
- `baseline_nodes.csv`: Per-node values at each step
- `baseline_events.csv`: Event log
- `baseline_incidents.csv`: Incident log (if any)
- `baseline_summary.json`: Computed statistics

---

## 5. Analysis Plan

### 5.1 Visual Inspection

**Plot 1: Time series**
```python
fig, axes = plt.subplots(3, 1, sharex=True)
axes[0].plot(steps, H)
axes[0].set_ylabel('H (Hamiltonian)')
axes[0].grid(True)

axes[1].plot(steps, T, label='T', color='orange')
axes[1].set_ylabel('Kinetic Energy T')
axes[1].grid(True)

axes[2].plot(steps, V, label='V', color='red')
axes[2].set_ylabel('Potential Energy V')
axes[2].set_xlabel('Time Step')
axes[2].grid(True)

plt.suptitle('Baseline: Energy vs Time')
plt.savefig('baseline_timeseries.png')
```

**Plot 2: Phase space**
```python
plt.scatter(T, V, c=steps, cmap='viridis', s=20)
plt.xlabel('T (Kinetic)')
plt.ylabel('V (Potential)')
plt.colorbar(label='Time Step')
plt.title('Phase Space Trajectory (T vs V)')
plt.savefig('baseline_phase_space.png')
```

**Plot 3: Field evolution**
```python
for node in nodes:
    plt.plot(steps, health[node], label=node)
plt.ylabel('Health')
plt.xlabel('Time Step')
plt.legend()
plt.title('Health Evolution')
plt.savefig('baseline_health_evolution.png')
```

### 5.2 Statistical Tests

**Test 1: Stationarity of H**

**Method**: Augmented Dickey-Fuller (ADF) test on H[20:] (exclude transient)

```python
from statsmodels.tss.stattools import adfuller

H_steady = H[20:]  # Exclude first 20 steps (transient)
result = adfuller(H_steady)

print(f"ADF statistic: {result[0]}")
print(f"p-value: {result[1]}")

# Interpretation:
# p < 0.05 → Reject unit root → H is stationary → PASS
# p >= 0.05 → Cannot reject unit root → H may have trend → FAIL
```

**Success criterion**: p < 0.05 (H is stationary)

**Test 2: Mean stability**

Check if mean(H) in first half ≈ mean(H) in second half.

```python
H_first_half = H[20:60]
H_second_half = H[60:100]

from scipy.stats import ttest_ind

t_stat, p_value = ttest_ind(H_first_half, H_second_half)

print(f"t-statistic: {t_stat}, p-value: {p_value}")

# Interpretation:
# p > 0.05 → Means are similar → Equilibrium → PASS
# p < 0.05 → Means differ → Drift/trend → FAIL
```

**Success criterion**: p > 0.05 (no significant difference between halves)

**Test 3: Bounded variance**

```python
std_H = np.std(H[20:])
mean_H = np.mean(H[20:])

cv = std_H / mean_H  # Coefficient of variation

print(f"Mean H: {mean_H:.3f}, Std H: {std_H:.3f}, CV: {cv:.3f}")

# Success: CV < 0.3 (gentle oscillations)
# Failure: CV > 0.5 (wild swings)
```

**Success criterion**: CV < 0.3

### 5.3 Summary Statistics

```python
summary = {
    'mean_H': np.mean(H[20:]),
    'std_H': np.std(H[20:]),
    'min_H': np.min(H[20:]),
    'max_H': np.max(H[20:]),
    'mean_T': np.mean(T[20:]),
    'mean_V': np.mean(V[20:]),
    'mean_T_over_V': np.mean(T[20:] / V[20:]),
    'total_incidents': len(incidents),
    'nodes_with_E_local_spike': count_E_local_spikes(E_local, threshold=0.5),
}

with open('baseline_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
```

---

## 6. Success Criteria

### 6.1 Quantitative Thresholds

**PASS if ALL of these hold**:

1. **Stationarity**: ADF p-value < 0.05 for H[20:]
2. **Mean stability**: t-test p-value > 0.05 (first half vs second half)
3. **Bounded oscillation**: CV(H) < 0.3
4. **Reasonable range**: 0.5 < mean(H) < 2.0 (not absurdly small or large)
5. **Finite values**: No NaN, inf, or negative energies
6. **Low incidents**: < 5 total incidents in 100 steps

**CONDITIONAL PASS if**:
- 4/6 criteria met AND failures are minor (e.g., CV = 0.35 instead of 0.3)

**FAIL if**:
- < 4/6 criteria met
- Any NaN/inf values
- H diverges (max(H) > 10)
- System crashes

### 6.2 Calibration Outputs

If PASS, use baseline to set:
```python
H_emergency = 2.0 * mean(H)  # Governance threshold
E_local_warning = mean(max(E_local per step)) + 2 * std(...)
T_frozen = 0.5 * mean(T)  # Below this = system frozen
```

---

## 7. Implementation Notes

### 7.1 Code to Write

**Phase 1** (see `mvp-implementation.md`):
- `core/graph_model.py`: TensegrityGraph class
- `core/energy.py`: Energy calculation functions
- `core/state.py`: SimulationState class

**Phase 2**:
- `events/field_events.py`: FeatureChange, Refactor
- `actors/feature_engineer.py`, etc.

**Phase 3**:
- `simulation.py`: Main loop
- `utils/logging_utils.py`: CSV/JSON logging

**Phase 4**:
- `scenarios/baseline.py`: This experiment

**Phase 5**:
- `utils/visualization.py`: Plotting functions
- `analysis/baseline_stats.py`: Statistical tests

### 7.2 Tests to Add

```python
# tests/test_baseline_experiment.py

def test_baseline_runs_without_crash():
    """Smoke test: baseline completes 100 steps."""
    scenario = create_baseline_scenario()
    state = SimulationState(scenario['graph'], **scenario['fields'])
    history = run_simulation(state, scenario['actors'],
                            scenario['scheduled'], n_steps=100)
    assert len(history) == 100

def test_baseline_energies_finite():
    """All energies are finite (no NaN, inf)."""
    history = run_baseline()
    for entry in history:
        assert np.isfinite(entry['H'])
        assert np.isfinite(entry['T'])
        assert np.isfinite(entry['V'])

def test_baseline_energies_nonnegative():
    """T and V are non-negative."""
    history = run_baseline()
    for entry in history:
        assert entry['T'] >= 0
        assert entry['V'] >= 0
```

### 7.3 Confusions to Resolve

Before running this experiment, resolve:
- **CONFUSION #1**: What is mass m_i? (affects T calculation)
- **CONFUSION #2**: How to compute risk field? (affects badness, affects all energies)
- **CONFUSION #4**: Incident probability function (affects incident count criterion)

See `confusions.md` for details.

---

## 8. Failure Modes and Contingencies

### Failure Mode 1: H diverges

**Symptoms**: H grows monotonically, max(H) > 10

**Diagnosis**:
- Actors adding complexity faster than refactors can remove
- Energy formula has bug (missing normalization?)
- Event magnitudes too large (Δcomplexity = 0.1 is too much?)

**Fix**:
1. Check energy calculation (hand-verify for 3-node case)
2. Reduce event magnitudes (Δcomplexity = 0.05)
3. Add more RefactorEngineers (2 refactor, 1 feature)
4. Debug actor policies (are they sampling correctly?)

### Failure Mode 2: H frozen (no dynamics)

**Symptoms**: H constant after step 5, T ≈ 0

**Diagnosis**:
- Actors not acting (policy returns null event?)
- Fields clamped to [0,1], saturated immediately
- Flow field is zero everywhere

**Fix**:
1. Debug actor `choose_action` (print events generated)
2. Check field clamping (are all values at 0 or 1?)
3. Verify flow field computation (print flow[i] values)

### Failure Mode 3: NaN or inf values

**Symptoms**: Energy calculations produce NaN or inf

**Diagnosis**:
- Division by zero (e.g., in normalization)
- Laplacian matrix singular (disconnected graph?)
- Overflow in energy calculation

**Fix**:
1. Add assertions: `assert np.isfinite(L).all()`
2. Check for disconnected components
3. Use float64 instead of float32

### Failure Mode 4: Wildly oscillating H

**Symptoms**: CV(H) > 0.5, chaotic swings

**Diagnosis**:
- Event magnitudes too large (system overshooting)
- Too many actors (too much simultaneous change)
- Positive feedback loop (bad code creates more bad code?)

**Fix**:
1. Reduce event magnitudes
2. Run with 1 actor only (simplest case)
3. Check if refactors actually reduce badness

---

## 9. Expected Outcomes

**If model is sane**:
- H oscillates gently around ~1.0
- T small (~0.01-0.05), V moderate (~0.8-1.2)
- Health stays in [0.5, 0.9]
- Complexity drifts slightly upward but bounded
- 0-3 incidents total

**What we learn**:
- Baseline behavior for comparison
- Typical ranges for H, T, V
- Calibration values for governance thresholds
- Which confusions matter (some may not affect outcomes)

**What we DON'T learn yet**:
- Whether E_local predicts anything (need Exp02 for that)
- Whether governance helps (need Exp03 for that)

---

## 10. Timeline

**Estimated effort**: 1-2 weeks (including Phase 1-4 implementation)

**Breakdown**:
- Phase 1 (core): 2-3 days
- Phase 2 (events/actors): 2-3 days
- Phase 3 (simulation): 1 day
- Phase 4 (baseline scenario): 1 day
- Phase 5 (analysis): 1 day
- Debugging/iteration: 2-3 days

**Deliverable**: Working baseline with plots, stats, and summary JSON.

**Go/no-go decision**: If baseline PASSES → proceed to Exp02. If FAILS → debug model before claiming anything.

---

## 11. References

- **MVP Scenarios**: `../simulation/mvp-scenarios.md` (Scenario 1: Baseline)
- **Implementation**: `../simulation/mvp-implementation.md` (Phases 1-5)
- **Model**: `../simulation/mvp-model.md` (Fields, energies)
- **Confusions**: `confusions.md` (Unresolved issues)

---

**Next**: Design Exp02 (Laplacian early warning) once baseline works.
